{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d7e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "sys.path.append(\"../NEAT\")\n",
    "from NEATModels import NEATDynamic, nets\n",
    "from NEATModels.config import dynamic_config\n",
    "from NEATUtils import helpers\n",
    "from NEATUtils.helpers import save_json, load_json\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb616cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "npz_directory = '/home/sancere/Kepler/CurieTrainingDatasets/oneatnpz/'\n",
    "npz_name = 'microbin2V1.npz'\n",
    "npz_val_name = 'microbin2V1val.npz'\n",
    "\n",
    "#Read and Write the h5 file, directory location and name\n",
    "model_dir =  '/home/sancere/Kepler/CurieDeepLearningModels/OneatModels/MicroscopeV1Models/'\n",
    "model_name = 'micronetbin2d29f16.h5'\n",
    "\n",
    "#Neural network parameters\n",
    "division_categories_json = model_dir + 'MicroscopeCategories.json'\n",
    "key_categories = load_json(division_categories_json)\n",
    "division_cord_json = model_dir + 'MicroscopeCord.json'\n",
    "key_cord = load_json(division_cord_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47812eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For ORNET use residual = True and for OSNET use residual = False\n",
    "residual = True\n",
    "#NUmber of starting convolutional filters, is doubled down with increasing depth\n",
    "startfilter = 16\n",
    "#CNN network start layer, mid layers and lstm layer kernel size\n",
    "start_kernel = 7\n",
    "lstm_kernel = 3\n",
    "mid_kernel = 3\n",
    "#Network depth has to be 9n + 2, n= 3 or 4 is optimal for Notum dataset\n",
    "depth = 29\n",
    "#Size of the gradient descent length vector, start small and use callbacks to get smaller when reaching the minima\n",
    "learning_rate = 1.0E-4\n",
    "#For stochastic gradient decent, the batch size used for computing the gradients\n",
    "batch_size = 8\n",
    "# use softmax for single event per box, sigmoid for multi event per box\n",
    "lstm_hidden_unit = 16\n",
    "#Training epochs, longer the better with proper chosen learning rate\n",
    "epochs = 250\n",
    "nboxes = 1\n",
    "#The inbuilt model stride which is equal to the nulber of times image was downsampled by the network\n",
    "show = False\n",
    "stage_number = 3\n",
    "last_conv_factor = 4\n",
    "size_tminus = 3\n",
    "size_tplus = 0\n",
    "imagex = 64\n",
    "imagey = 64\n",
    "yolo_v0 = False\n",
    "yolo_v1 = True\n",
    "yolo_v2 = False\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "config = dynamic_config(npz_directory =npz_directory, npz_name = npz_name, npz_val_name = npz_val_name, \n",
    "                         key_categories = key_categories, key_cord = key_cord, nboxes = nboxes, imagex = imagex,\n",
    "                         imagey = imagey, size_tminus = size_tminus, size_tplus =size_tplus, epochs = epochs, yolo_v0 = yolo_v0, yolo_v1 = yolo_v1, yolo_v2 = yolo_v2,\n",
    "                         residual = residual, depth = depth, start_kernel = start_kernel, mid_kernel = mid_kernel, stage_number = stage_number, last_conv_factor = last_conv_factor,\n",
    "                         lstm_kernel = lstm_kernel, lstm_hidden_unit = lstm_hidden_unit, show = show,\n",
    "                         startfilter = startfilter, batch_size = batch_size, model_name = model_name)\n",
    "\n",
    "config_json = config.to_json()\n",
    "\n",
    "print(config)\n",
    "save_json(config_json, model_dir + os.path.splitext(model_name)[0] + '_Parameter.json')\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "Train = NEATDynamic(config, model_dir, model_name)\n",
    "\n",
    "Train.loadData()\n",
    "\n",
    "Train.TrainModel()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowGPU] *",
   "language": "python",
   "name": "conda-env-tensorflowGPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
